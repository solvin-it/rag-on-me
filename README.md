# ðŸ“š rag-on-me â€” Minimal RAG API (FastAPI â€¢ LangGraph â€¢ pgvector)

*A small, clean Retrieval-Augmented Generation (RAG) service I built to show end-to-end RAG: ingestion â†’ retrieval â†’ generation, with conversation state stored in Postgres.*

---

## ðŸš€ Introduction

I built **rag-on-me** to learn (and show) how the moving parts of a RAG system fit together without tons of framework magic. It wires **FastAPI** to a **LangGraph** pipeline, stores embeddings in **Postgres + pgvector**, and uses **OpenAI** for chat + embeddings.

What it demonstrates:

* Ingesting markdown into a vector store
* A simple **retrieve â†’ generate** graph with LangGraph
* **Checkpointed conversations** (multi-turn state in Postgres)

Itâ€™s intentionally compact and easy to read so you can follow the flow from HTTP request to LLM response.

---

## ðŸŒ Live Demo

ðŸ‘‰ **[SOON]**

---

## ðŸŽ¯ Features

* **RAG in under 1K lines** â€” minimal glue, maximum clarity
* **Markdown ingestion** â€” load `cv.md` into pgvector (easy to swap sources)
* **Threaded chat** â€” each `thread_id` keeps its own conversation state
* **Postgres-backed checkpoints** â€” resume conversations reliably
* **Batteries-included HTTP** â€” `POST /initialize`, `POST /chat`, `GET /graph/state`

---

## ðŸ§­ How It Works (High Level)

**Client â†’ FastAPI â†’ LangGraph â†’ Vector Store + LLM â†’ Postgres (checkpoints)**

Key modules:

* `app/main.py` â€” FastAPI app, startup lifecycle, graph compile
* `app/modules/rag/graph_runtime.py` â€” graph wiring (retrieve â†’ generate)
* `app/modules/rag/nodes.py` â€” retrieval + generation nodes
* `app/modules/rag/adapters.py` â€” singletons for LLM, embeddings, vector store
* `app/modules/ingest/ingest.py` â€” markdown ingestion utilities

---

## ðŸ› ï¸ Setup (Local)

**Requirements:** Python 3.12, Docker (for Postgres + pgvector)

1. Start the database

```bash
docker compose --profile local -f docker/compose.yaml up -d
```

2. Configure environment

```bash
cp .env.example .env
# fill in: OPENAI_API_KEY, POSTGRES_* â€¦
```

3. Run the API

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

4. (Optional) Ingest the sample CV

```bash
curl -X POST "http://localhost:8000/initialize" \
  -H "Content-Type: application/json" \
  -d '{"file_name":"cv.md"}'
```

5. Chat with the graph

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"human","content":"Who is Jose?"}], "thread_id":"demo-1"}'
```

---

## âš™ï¸ Configuration

* `OPENAI_API_KEY` â€” your OpenAI key
* `OPENAI_CHAT_MODEL` â€” default chat model **[NEEDS CONFIRMATION: e.g., gpt-5-nano]**
* `OPENAI_EMBEDDING_MODEL` â€” default embeddings (currently `text-embedding-3-large`)
* `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB`, `POSTGRES_HOST`, `POSTGRES_PORT`
* `LANGSMITH_TRACING`, `LANGSMITH_API_KEY` â€” optional tracing **[NEEDS CONFIRMATION]**

---

## ðŸ–¥ï¸ API (Quick Reference)

* `POST /health` â†’ `{ "status": "healthy" }`
* `POST /initialize` â†’ `{ "file_name": "cv.md" }` (ingests the doc into pgvector)
* `POST /chat` â†’ send messages + optional `thread_id`

**Example request**

```json
{
  "messages": [
    { "role": "human", "content": "Tell me about the CV." }
  ],
  "thread_id": "demo-1"
}
```

**Example response** (**shape simplified**)

```json
{
  "output": {
    "message": "Jose is a software engineer with..."
  }
}
```

> **[NEEDS CONFIRMATION]** The code may currently return `{"output": {"messages": "<string>"}}`. I plan to standardize on `{"message": "<string>"}` in a near-term update.

---

## ðŸ“‚ Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”œâ”€â”€ core/config.py          # env + DB helpers
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ ingest/             # ingestion utilities
â”‚   â”‚   â””â”€â”€ rag/                # graph, nodes, adapters, schemas
â”‚   â””â”€â”€ sources/                # sample docs (cv.md)
â”œâ”€â”€ docker/compose.yaml         # local Postgres + pgvector
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ backlogs.md                 # improvement plan
â””â”€â”€ README.md
```

---

## ðŸ”Ž Notes & Conventions

* **Singleton clients** via `@lru_cache` for LLM, embeddings, and vector store
* **LangGraph `StateGraph`** using `MessagesState` (messages live at `"messages"`)
* **Checkpointing** via LangGraph `PostgresSaver` (psycopg-style DSN)
* **Ingestion defaults**: chunk size ~1000, overlap 100 **[NEEDS CONFIRMATION]**

---

## ðŸ”­ Roadmap

Short, high-impact updates I plan to tackle next:

* **Input validation & response shape** â€” normalize roles, cap sizes, and standardize `{"message": ...}`
* **History trimming** â€” rolling window + (optional) summary for long chats
* **Async end-to-end** â€” convert handlers + clients to async for better concurrency
* **Streaming** â€” `/chat/stream` with SSE for token-by-token replies
* **Retrieval tuning** â€” `k`, namespace filters, dedupe + pre-formatted context
* **Idempotent ingestion** â€” prevent duplicate vectors on re-runs
* **Security & ops** â€” tighter CORS, optional API key, basic rate limits **[NEEDS CONFIRMATION]**
* **Observability** â€” request IDs in logs; tokens/latency metrics **[NEEDS CONFIRMATION: Prometheus + OpenTelemetry]**

---

## ðŸ“„ License

MIT

---

## âœ¨ Recruiter-Friendly Closing Note

I built **rag-on-me** to show that I can **design and ship** more than a notebook demo. Itâ€™s a production-flavored RAG slice: clean API, clear data flow, and deliberate trade-offs. The value isnâ€™t just that it answers questionsâ€”itâ€™s that the code makes it easy to **extend** (streaming, better retrieval, stricter validation) and **operate** (checkpoints, logs, config).

If youâ€™re evaluating me for a role that touches **LLM apps, platform, or ML tooling**, this repo reflects how I think: start simple, keep it readable, measure what matters, and iterate with intent.